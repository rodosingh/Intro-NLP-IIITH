{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-09 22:46:51.160437: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-09 22:46:51.160483: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2472it [00:14, 175.97it/s]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for dirname, _, filenames in tqdm(os.walk('./')):\n",
    "    for filename in filenames:\n",
    "        if os.path.join(dirname, filename).endswith('trans.text'):\n",
    "            with open(os.path.join(dirname, filename), 'r') as f:\n",
    "                for line in f:\n",
    "                    if len(line.split(' '))>3:\n",
    "                        line = line.strip().split(' ')[3:]\n",
    "                        if len(line)>2:\n",
    "                            t = ' '.join(line)\n",
    "                            t = t.replace('[noise]', ' ').replace('[silence]', ' ')\n",
    "                            t = re.sub(r'[^\\w\\s]', '', t)\n",
    "                            data.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168829, 'i am actually a junior beginning of my junior year')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data), data[2314]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(data)\n",
    "tokenized_text = tokenizer.texts_to_sequences(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-05T04:51:46.509028Z",
     "iopub.status.busy": "2021-11-05T04:51:46.508772Z",
     "iopub.status.idle": "2021-11-05T04:51:46.763989Z",
     "shell.execute_reply": "2021-11-05T04:51:46.76327Z",
     "shell.execute_reply.started": "2021-11-05T04:51:46.508993Z"
    }
   },
   "outputs": [],
   "source": [
    "train1, train2 = train_test_split(tokenized_text, test_size=0.5, random_state=42)\n",
    "train11, train12 = train_test_split(train1, test_size=0.5, random_state=42)\n",
    "train21, train22 = train_test_split(train2, test_size=0.5, random_state=42)\n",
    "\n",
    "t1, t2 = train_test_split(train11, test_size=0.5, random_state=42)\n",
    "t3, t4 = train_test_split(train12, test_size=0.5, random_state=42)\n",
    "t5, t6 = train_test_split(train21, test_size=0.5, random_state=42)\n",
    "t7, t8 = train_test_split(train22, test_size=0.5, random_state=42)\n",
    "\n",
    "train_tokenized_text1, val_tokenized_text1 = train_test_split(t1, test_size=0.1, random_state=42)\n",
    "train_tokenized_text2, val_tokenized_text2 = train_test_split(t2, test_size=0.1, random_state=42)\n",
    "train_tokenized_text3, val_tokenized_text3 = train_test_split(t3, test_size=0.1, random_state=42)\n",
    "train_tokenized_text4, val_tokenized_text4 = train_test_split(t4, test_size=0.1, random_state=42)\n",
    "train_tokenized_text5, val_tokenized_text5 = train_test_split(t5, test_size=0.1, random_state=42)\n",
    "train_tokenized_text6, val_tokenized_text6 = train_test_split(t6, test_size=0.1, random_state=42)\n",
    "train_tokenized_text7, val_tokenized_text7 = train_test_split(t7, test_size=0.1, random_state=42)\n",
    "train_tokenized_text8, val_tokenized_text8 = train_test_split(t8, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-05T04:51:46.765629Z",
     "iopub.status.busy": "2021-11-05T04:51:46.765273Z",
     "iopub.status.idle": "2021-11-05T04:51:47.449201Z",
     "shell.execute_reply": "2021-11-05T04:51:47.448218Z",
     "shell.execute_reply.started": "2021-11-05T04:51:46.765594Z"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-05T04:51:47.451275Z",
     "iopub.status.busy": "2021-11-05T04:51:47.450945Z",
     "iopub.status.idle": "2021-11-05T04:51:47.724364Z",
     "shell.execute_reply": "2021-11-05T04:51:47.723623Z",
     "shell.execute_reply.started": "2021-11-05T04:51:47.451228Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(f'data/train_tokenized_text1.pickle', 'wb') as handle:\n",
    "    pickle.dump({f'train_tokenized_text1':train_tokenized_text1}, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(f'data/train_tokenized_text2.pickle', 'wb') as handle:\n",
    "    pickle.dump({f'train_tokenized_text2':train_tokenized_text2}, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(f'data/train_tokenized_text3.pickle', 'wb') as handle:\n",
    "    pickle.dump({f'train_tokenized_text3':train_tokenized_text3}, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(f'data/train_tokenized_text4.pickle', 'wb') as handle:\n",
    "    pickle.dump({f'train_tokenized_text4':train_tokenized_text4}, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(f'data/train_tokenized_text5.pickle', 'wb') as handle:\n",
    "    pickle.dump({f'train_tokenized_text5':train_tokenized_text5}, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(f'data/train_tokenized_text6.pickle', 'wb') as handle:\n",
    "    pickle.dump({f'train_tokenized_text6':train_tokenized_text6}, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(f'data/train_tokenized_text7.pickle', 'wb') as handle:\n",
    "    pickle.dump({f'train_tokenized_text7': train_tokenized_text7}, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(f'data/train_tokenized_text8.pickle', 'wb') as handle:\n",
    "    pickle.dump({f'train_tokenized_text8': train_tokenized_text8}, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(f'data/val_tokenized_text1.pickle', 'wb') as handle:\n",
    "    pickle.dump({f'val_tokenized_text1':val_tokenized_text1}, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(f'data/val_tokenized_text2.pickle', 'wb') as handle:\n",
    "    pickle.dump({f'val_tokenized_text2':val_tokenized_text2}, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(f'data/val_tokenized_text3.pickle', 'wb') as handle:\n",
    "    pickle.dump({f'val_tokenized_text3':val_tokenized_text3}, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(f'data/val_tokenized_text4.pickle', 'wb') as handle:\n",
    "    pickle.dump({f'val_tokenized_text4':val_tokenized_text4}, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(f'data/val_tokenized_text5.pickle', 'wb') as handle:\n",
    "    pickle.dump({f'val_tokenized_text5':val_tokenized_text5}, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(f'data/val_tokenized_text6.pickle', 'wb') as handle:\n",
    "    pickle.dump({f'val_tokenized_text6':val_tokenized_text6}, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(f'data/val_tokenized_text7.pickle', 'wb') as handle:\n",
    "    pickle.dump({f'val_tokenized_text7': val_tokenized_text7}, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(f'data/val_tokenized_text8.pickle', 'wb') as handle:\n",
    "    pickle.dump({f'val_tokenized_text8': val_tokenized_text8}, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-05T04:51:47.726154Z",
     "iopub.status.busy": "2021-11-05T04:51:47.725846Z",
     "iopub.status.idle": "2021-11-05T04:51:47.733173Z",
     "shell.execute_reply": "2021-11-05T04:51:47.732385Z",
     "shell.execute_reply.started": "2021-11-05T04:51:47.726116Z"
    }
   },
   "outputs": [],
   "source": [
    "len(train_tokenized_text1), len(train_tokenized_text2), len(train_tokenized_text3), len(train_tokenized_text4), len(train_tokenized_text5), len(train_tokenized_text6), len(train_tokenized_text7), len(train_tokenized_text8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-05T04:51:47.735106Z",
     "iopub.status.busy": "2021-11-05T04:51:47.734645Z",
     "iopub.status.idle": "2021-11-05T04:51:47.748272Z",
     "shell.execute_reply": "2021-11-05T04:51:47.747465Z",
     "shell.execute_reply.started": "2021-11-05T04:51:47.735066Z"
    }
   },
   "outputs": [],
   "source": [
    "len(val_tokenized_text1), len(val_tokenized_text2), len(val_tokenized_text3), len(val_tokenized_text4), len(val_tokenized_text5), len(val_tokenized_text6), len(val_tokenized_text7), len(val_tokenized_text8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-05T04:51:47.750035Z",
     "iopub.status.busy": "2021-11-05T04:51:47.74975Z",
     "iopub.status.idle": "2021-11-05T04:51:47.759633Z",
     "shell.execute_reply": "2021-11-05T04:51:47.758986Z",
     "shell.execute_reply.started": "2021-11-05T04:51:47.749998Z"
    }
   },
   "outputs": [],
   "source": [
    "len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_training_data(tokenized_text):\n",
    "    for _, sentence in enumerate(tokenized_text):\n",
    "        if len(sentence)>2:\n",
    "            for i in range(len(sentence)):\n",
    "                inp = sentence[:i]+sentence[i+1:]\n",
    "                inp1 = tf.keras.preprocessing.sequence.pad_sequences([inp], padding='pre', maxlen=64)\n",
    "                inp2 = tf.keras.preprocessing.sequence.pad_sequences([inp[::-1]], padding='post', maxlen=64)\n",
    "                target = tf.keras.utils.to_categorical(sentence[i], 29271).reshape(1,29271)\n",
    "                yield (inp1, inp2), target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-09 22:47:49.243110: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-11-09 22:47:49.243158: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-11-09 22:47:49.243184: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (dhaval-Latitude-3560): /proc/driver/nvidia/version does not exist\n",
      "2021-11-09 22:47:49.243439: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "inp1 = tf.keras.Input(shape=(64,))\n",
    "inp2 = tf.keras.Input(shape=(64,))\n",
    "\n",
    "embed1 = tf.keras.layers.Embedding(input_dim=29271, output_dim=64, input_length=81, mask_zero=True)(inp1)\n",
    "embed2 = tf.keras.layers.Embedding(input_dim=29271, output_dim=64, input_length=81, mask_zero=True)(inp2)\n",
    "\n",
    "output11, h11, c11 = tf.keras.layers.LSTM(64, dropout=0.1,return_sequences=True, return_state=True)(embed1)\n",
    "output12, h12, c12 = tf.keras.layers.LSTM(64, dropout=0.1,return_sequences=True, return_state=True)(output11)\n",
    "\n",
    "output21, h21, c21 = tf.keras.layers.LSTM(64, dropout=0.1,return_sequences=True, return_state=True)(embed2)\n",
    "output22, h2, c22 = tf.keras.layers.LSTM(64, dropout=0.1,return_sequences=True, return_state=True)(output21)\n",
    "\n",
    "embed = tf.keras.layers.concatenate([embed1, embed2], axis=-1)\n",
    "concat1 = tf.keras.layers.concatenate([c11, c21], axis=-1)\n",
    "concat2 = tf.keras.layers.concatenate([c12, c22], axis=-1)\n",
    "\n",
    "w1 = tf.Variable(0.2, trainable=True)\n",
    "w2 = tf.Variable(0.3, trainable=True)\n",
    "w3 = tf.Variable(0.5, trainable=True)\n",
    "\n",
    "pre_out = w1*embed[:, -1, :] + w2*concat1 + w3*concat2\n",
    "\n",
    "out = tf.keras.layers.Dense(29271, activation='softmax')(pre_out)\n",
    "\n",
    "elmo = tf.keras.Model(inputs=[inp1, inp2], outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 64, 64)       1873344     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 64, 64)       1873344     ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 64, 128)      0           ['embedding[0][0]',              \n",
      "                                                                  'embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 64, 64),     33024       ['embedding[0][0]']              \n",
      "                                 (None, 64),                                                      \n",
      "                                 (None, 64)]                                                      \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)                  [(None, 64, 64),     33024       ['embedding_1[0][0]']            \n",
      "                                 (None, 64),                                                      \n",
      "                                 (None, 64)]                                                      \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None, 128)         0           ['concatenate[0][0]']            \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 128)          0           ['lstm[0][2]',                   \n",
      "                                                                  'lstm_2[0][2]']                 \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, 64, 64),     33024       ['lstm[0][0]']                   \n",
      "                                 (None, 64),                                                      \n",
      "                                 (None, 64)]                                                      \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)                  [(None, 64, 64),     33024       ['lstm_2[0][0]']                 \n",
      "                                 (None, 64),                                                      \n",
      "                                 (None, 64)]                                                      \n",
      "                                                                                                  \n",
      " tf.math.multiply (TFOpLambda)  (None, 128)          0           ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_1 (TFOpLambda  (None, 128)         0           ['concatenate_1[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 128)          0           ['lstm_1[0][2]',                 \n",
      "                                                                  'lstm_3[0][2]']                 \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 128)         0           ['tf.math.multiply[0][0]',       \n",
      " da)                                                              'tf.math.multiply_1[0][0]']     \n",
      "                                                                                                  \n",
      " tf.math.multiply_2 (TFOpLambda  (None, 128)         0           ['concatenate_2[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 128)         0           ['tf.__operators__.add[0][0]',   \n",
      " mbda)                                                            'tf.math.multiply_2[0][0]']     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 29271)        3775959     ['tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 7,654,743\n",
      "Trainable params: 7,654,743\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "elmo.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])\n",
    "elmo.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "elmo.load_weights('./weights.best.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T09:29:59.77376Z",
     "iopub.status.busy": "2021-11-06T09:29:59.77323Z",
     "iopub.status.idle": "2021-11-06T09:29:59.778156Z",
     "shell.execute_reply": "2021-11-06T09:29:59.777237Z",
     "shell.execute_reply.started": "2021-11-06T09:29:59.77372Z"
    }
   },
   "outputs": [],
   "source": [
    "# checkpoint_filepath = './weights.best.hdf5'\n",
    "checkpoint_filepath  = 'weights.best.{epoch:01d}.hdf5'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n",
    "                                                                verbose = 1, \n",
    "                                                                monitor = 'val_accuracy',\n",
    "                                                                save_best_only = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T09:31:07.280962Z",
     "iopub.status.busy": "2021-11-06T09:31:07.28041Z",
     "iopub.status.idle": "2021-11-06T09:31:07.332003Z",
     "shell.execute_reply": "2021-11-06T09:31:07.331329Z",
     "shell.execute_reply.started": "2021-11-06T09:31:07.280923Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('../input/tokenized-data-elmo/train_tokenized_text1.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    train_tokenized_text = data['train_tokenized_text1']\n",
    "\n",
    "with open('../input/tokenized-data-elmo/val_tokenized_text1.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    val_tokenized_text = data['val_tokenized_text1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T09:31:07.863614Z",
     "iopub.status.busy": "2021-11-06T09:31:07.86301Z"
    }
   },
   "outputs": [],
   "source": [
    "elmo.fit(get_training_data(train_tokenized_text),\n",
    "          validation_data = get_training_data(val_tokenized_text),\n",
    "          batch_size=8,\n",
    "          validation_batch_size=4,\n",
    "          callbacks=[model_checkpoint_callback],\n",
    "          epochs=4,\n",
    "          steps_per_epoch=10000,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inp1 = tf.keras.Input(shape=(64,))\n",
    "inp2 = tf.keras.Input(shape=(64,))\n",
    "\n",
    "embed1 = tf.keras.layers.Embedding(input_dim=29271, output_dim=64, input_length=81, mask_zero=True)(inp1)\n",
    "embed2 = tf.keras.layers.Embedding(input_dim=29271, output_dim=64, input_length=81, mask_zero=True)(inp2)\n",
    "\n",
    "output11, h11, c11 = tf.keras.layers.LSTM(64, dropout=0.1,return_sequences=True, return_state=True)(embed1)\n",
    "output12, h12, c12 = tf.keras.layers.LSTM(64, dropout=0.1,return_sequences=True, return_state=True)(output11)\n",
    "\n",
    "output21, h21, c21 = tf.keras.layers.LSTM(64, dropout=0.1,return_sequences=True, return_state=True)(embed2)\n",
    "output22, h2, c22 = tf.keras.layers.LSTM(64, dropout=0.1,return_sequences=True, return_state=True)(output21)\n",
    "\n",
    "embed = tf.keras.layers.concatenate([embed1, embed2], axis=-1)\n",
    "concat1 = tf.keras.layers.concatenate([c11, c21], axis=-1)\n",
    "concat2 = tf.keras.layers.concatenate([c12, c22], axis=-1)\n",
    "\n",
    "output = 0.4*embed[:, -1, :] + 0.3*concat1 + 0.3*concat2\n",
    "\n",
    "model = tf.keras.Model(inputs=[inp1, inp2], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, 64, 64)       1873344     ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, 64, 64)       1873344     ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 64, 128)      0           ['embedding_2[0][0]',            \n",
      "                                                                  'embedding_3[0][0]']            \n",
      "                                                                                                  \n",
      " lstm_4 (LSTM)                  [(None, 64, 64),     33024       ['embedding_2[0][0]']            \n",
      "                                 (None, 64),                                                      \n",
      "                                 (None, 64)]                                                      \n",
      "                                                                                                  \n",
      " lstm_6 (LSTM)                  [(None, 64, 64),     33024       ['embedding_3[0][0]']            \n",
      "                                 (None, 64),                                                      \n",
      "                                 (None, 64)]                                                      \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1 (Sl  (None, 128)         0           ['concatenate_3[0][0]']          \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 128)          0           ['lstm_4[0][2]',                 \n",
      "                                                                  'lstm_6[0][2]']                 \n",
      "                                                                                                  \n",
      " lstm_5 (LSTM)                  [(None, 64, 64),     33024       ['lstm_4[0][0]']                 \n",
      "                                 (None, 64),                                                      \n",
      "                                 (None, 64)]                                                      \n",
      "                                                                                                  \n",
      " lstm_7 (LSTM)                  [(None, 64, 64),     33024       ['lstm_6[0][0]']                 \n",
      "                                 (None, 64),                                                      \n",
      "                                 (None, 64)]                                                      \n",
      "                                                                                                  \n",
      " tf.math.multiply_3 (TFOpLambda  (None, 128)         0           ['tf.__operators__.getitem_1[0][0\n",
      " )                                                               ]']                              \n",
      "                                                                                                  \n",
      " tf.math.multiply_4 (TFOpLambda  (None, 128)         0           ['concatenate_4[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 128)          0           ['lstm_5[0][2]',                 \n",
      "                                                                  'lstm_7[0][2]']                 \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 128)         0           ['tf.math.multiply_3[0][0]',     \n",
      " mbda)                                                            'tf.math.multiply_4[0][0]']     \n",
      "                                                                                                  \n",
      " tf.math.multiply_5 (TFOpLambda  (None, 128)         0           ['concatenate_5[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 128)         0           ['tf.__operators__.add_2[0][0]', \n",
      " mbda)                                                            'tf.math.multiply_5[0][0]']     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,878,784\n",
      "Trainable params: 3,878,784\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.set_weights([elmo.layers[2].get_weights()[0],\n",
    "                   elmo.layers[3].get_weights()[0], \n",
    "                   elmo.layers[5].get_weights()[0],\n",
    "                   elmo.layers[5].get_weights()[1],\n",
    "                   elmo.layers[5].get_weights()[2],\n",
    "                   elmo.layers[6].get_weights()[0],\n",
    "                   elmo.layers[6].get_weights()[1],\n",
    "                   elmo.layers[6].get_weights()[2],\n",
    "                   elmo.layers[9].get_weights()[0],\n",
    "                   elmo.layers[9].get_weights()[1],\n",
    "                   elmo.layers[9].get_weights()[2],\n",
    "                   elmo.layers[10].get_weights()[0],\n",
    "                   elmo.layers[10].get_weights()[1],\n",
    "                   elmo.layers[10].get_weights()[2],\n",
    "                  ]\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine distance is -  0.02296280860900879\n",
      "manhattan distance is -  10.061333\n"
     ]
    }
   ],
   "source": [
    "#Word 1 - Bank\n",
    "\n",
    "sent1 = tokenizer.texts_to_sequences(['I am going to river'])\n",
    "sent2 = tokenizer.texts_to_sequences(['of Ganga'])\n",
    "word1 = model.predict([tf.keras.preprocessing.sequence.pad_sequences(sent1, padding='pre', maxlen=64),\n",
    "               tf.keras.preprocessing.sequence.pad_sequences(sent2, padding='post', maxlen=64)])\n",
    "\n",
    "sent1 = tokenizer.texts_to_sequences(['I am going to '])\n",
    "sent2 = tokenizer.texts_to_sequences(['of India'])\n",
    "\n",
    "word2 = model.predict([tf.keras.preprocessing.sequence.pad_sequences(sent1, padding='pre', maxlen=64),\n",
    "               tf.keras.preprocessing.sequence.pad_sequences(sent2, padding='post', maxlen=64)])\n",
    "\n",
    "print('cosine distance is - ', scipy.spatial.distance.cosine(word1, word2))\n",
    "\n",
    "print('manhattan distance is - ', scipy.spatial.distance.cityblock(word1, word2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine distance is -  0.03328758478164673\n",
      "manhattan distance is -  10.628183\n"
     ]
    }
   ],
   "source": [
    "#Word 1 - mobile, #Word2 - laptop\n",
    "\n",
    "word1 = model.predict([tf.keras.preprocessing.sequence.pad_sequences([tokenized_text[2905]], padding='pre', maxlen=64),\n",
    "               tf.keras.preprocessing.sequence.pad_sequences([tokenized_text[2905][::-1]], padding='post', maxlen=64)])\n",
    "\n",
    "word2 = model.predict([tf.keras.preprocessing.sequence.pad_sequences([tokenized_text[6945]], padding='pre', maxlen=64),\n",
    "               tf.keras.preprocessing.sequence.pad_sequences([tokenized_text[6945][::-1]], padding='post', maxlen=64)])\n",
    "\n",
    "print('cosine distance is - ', scipy.spatial.distance.cosine(word1, word2))\n",
    "\n",
    "print('manhattan distance is - ', scipy.spatial.distance.cityblock(word1, word2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine distance is -  0.07780790328979492\n",
      "manhattan distance is -  14.03693\n"
     ]
    }
   ],
   "source": [
    "#Word 1 - vegetable, #Word2 - wood\n",
    "\n",
    "word1 = model.predict([tf.keras.preprocessing.sequence.pad_sequences([tokenized_text[2435]], padding='pre', maxlen=64),\n",
    "               tf.keras.preprocessing.sequence.pad_sequences([tokenized_text[2435][::-1]], padding='post', maxlen=64)])\n",
    "\n",
    "word2 = model.predict([tf.keras.preprocessing.sequence.pad_sequences([tokenized_text[1035]], padding='pre', maxlen=64),\n",
    "               tf.keras.preprocessing.sequence.pad_sequences([tokenized_text[1035][::-1]], padding='post', maxlen=64)])\n",
    "\n",
    "print('cosine distance is - ', scipy.spatial.distance.cosine(word1, word2))\n",
    "\n",
    "print('manhattan distance is - ', scipy.spatial.distance.cityblock(word1, word2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine distance is -  0.02940833568572998\n",
      "manhattan distance is -  10.147203\n"
     ]
    }
   ],
   "source": [
    "#Word 1 - bird, #Word2 - boat\n",
    "\n",
    "word1 = model.predict([tf.keras.preprocessing.sequence.pad_sequences([tokenized_text[1784]], padding='pre', maxlen=64),\n",
    "               tf.keras.preprocessing.sequence.pad_sequences([tokenized_text[1784][::-1]], padding='post', maxlen=64)])\n",
    "\n",
    "word2 = model.predict([tf.keras.preprocessing.sequence.pad_sequences([tokenized_text[962]], padding='pre', maxlen=64),\n",
    "               tf.keras.preprocessing.sequence.pad_sequences([tokenized_text[962][::-1]], padding='post', maxlen=64)])\n",
    "\n",
    "print('cosine distance is - ', scipy.spatial.distance.cosine(word1, word2))\n",
    "\n",
    "print('manhattan distance is - ', scipy.spatial.distance.cityblock(word1, word2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine distance is -  0.07126075029373169\n",
      "manhattan distance is -  14.171004\n"
     ]
    }
   ],
   "source": [
    "#Word 1 - drink, #Word2 - tie\n",
    "\n",
    "word1 = model.predict([tf.keras.preprocessing.sequence.pad_sequences([tokenized_text[1466]], padding='pre', maxlen=64),\n",
    "               tf.keras.preprocessing.sequence.pad_sequences([tokenized_text[1466][::-1]], padding='post', maxlen=64)])\n",
    "\n",
    "word2 = model.predict([tf.keras.preprocessing.sequence.pad_sequences([tokenized_text[2057]], padding='pre', maxlen=64),\n",
    "               tf.keras.preprocessing.sequence.pad_sequences([tokenized_text[2057][::-1]], padding='post', maxlen=64)])\n",
    "\n",
    "print('cosine distance is - ', scipy.spatial.distance.cosine(word1, word2))\n",
    "\n",
    "print('manhattan distance is - ', scipy.spatial.distance.cityblock(word1, word2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
